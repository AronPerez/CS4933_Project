{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_GAN.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOSeqfhBkFJsMvf5pfNiSPn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AronPerez/CS4933_Project/blob/aaron/CV_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYWG4t9bDsJb"
      },
      "source": [
        "# Initial "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaCzvQgnxV3Z"
      },
      "source": [
        "## Aarons workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugjlyN850yz5"
      },
      "source": [
        "### Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY6QQAlxLXTj",
        "outputId": "03814999-d0a7-4de4-bcaf-e09b8589c551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install --upgrade --force-reinstall tensorflow-gpu"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/cc/a27e73cf8b23f2ce4bdd2b7089a42a7819ce6dd7366dceba406ddc5daa9c/tensorflow_gpu-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3MB 31kB/s \n",
            "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hCollecting six~=1.15.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
            "Collecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Collecting h5py~=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 56.7MB/s \n",
            "\u001b[?25hCollecting wheel~=0.35\n",
            "  Downloading https://files.pythonhosted.org/packages/65/63/39d04c74222770ed1589c0eaba06c05891801219272420b40311cd60c880/wheel-0.36.2-py2.py3-none-any.whl\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
            "Collecting absl-py~=0.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/c9/ef0fae29182d7a867d203f0eff8296b60da92098cc41db33a434f4be84bf/absl_py-0.12.0-py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 58.6MB/s \n",
            "\u001b[?25hCollecting grpcio~=1.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/54/1c8be62beafe7fb1548d2968e518ca040556b46b0275399d4f3186c56d79/grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 53.8MB/s \n",
            "\u001b[?25hCollecting numpy~=1.19.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/d6/a6aaa29fea945bc6c61d11f6e0697b325ff7446de5ffd62c2fa02f627048/numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8MB 56.7MB/s \n",
            "\u001b[?25hCollecting termcolor~=1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
            "Collecting opt-einsum~=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n",
            "\u001b[?25hCollecting google-pasta~=0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
            "\u001b[?25hCollecting tensorboard~=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0MB 56.6MB/s \n",
            "\u001b[?25hCollecting astunparse~=1.6.3\n",
            "  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 57.0MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/4e/de63de3cd9a83d3c1753a4566b11fc9d90b845f2448a132cfd36d3cb3cd1/protobuf-3.15.8-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 55.2MB/s \n",
            "\u001b[?25hCollecting wrapt~=1.12.1\n",
            "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
            "Collecting werkzeug>=0.11.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 51.7MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/d3/7541e89f1fc456eef157224f597a8bba22589db6369a03eaba68c11f07a0/google_auth_oauthlib-0.4.4-py2.py3-none-any.whl\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/c1/499e600ba0c618b451cd9c425ae1c177249940a2086316552fee7d86c954/tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 55.5MB/s \n",
            "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/63/d92b4bc44261b7396558c054f78acf71468b5628bcb14cdaeb2504ea80d3/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 56.5MB/s \n",
            "\u001b[?25hCollecting requests<3,>=2.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.4MB/s \n",
            "\u001b[?25hCollecting setuptools>=41.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/42/2876a3a136f8bfa9bd703518441c8db78ff1eeaddf174baa85c083c1fd15/setuptools-56.0.0-py3-none-any.whl (784kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 57.7MB/s \n",
            "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/c1/44179a1cfc5c3b5832a5f9c925161612471ec5f346bcd186235651d74f35/google_auth-1.30.0-py2.py3-none-any.whl (146kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 59.4MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/33/1ae0f71395e618d6140fbbc9587cc3156591f748226075e0f7d6f9176522/Markdown-3.3.4-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.1MB/s \n",
            "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
            "Collecting certifi>=2017.4.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/a0/5f06e1e1d463903cf0c0eebeb751791119ed7a4b3737fdc9a77f1cdfb51f/certifi-2020.12.5-py2.py3-none-any.whl (147kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 56.5MB/s \n",
            "\u001b[?25hCollecting idna<3,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.5MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c6/d3e3abe5b4f4f16cf0dfc9240ab7ce10c2baa0e268989a4e3ec19e90c84e/urllib3-1.26.4-py2.py3-none-any.whl (153kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 58.1MB/s \n",
            "\u001b[?25hCollecting chardet<5,>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/c7/fa589626997dd07bd87d9269342ccb74b1720384a4d739a1872bd84fbe68/chardet-4.0.0-py2.py3-none-any.whl (178kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 61.1MB/s \n",
            "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 58.7MB/s \n",
            "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/28/c4f5796c67ad06bb91d98d543a5e01805c1ff065e08871f78e52d2a331ad/cachetools-4.2.2-py3-none-any.whl\n",
            "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/93/0c0f002031f18b53af7a6166103c02b9c0667be528944137cc954ec921b3/rsa-4.7.2-py3-none-any.whl\n",
            "Collecting importlib-metadata; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/e2/49966924c93909d47612bb47d911448140a2f6c1390aec2f4c1afbe3748f/importlib_metadata-4.0.1-py3-none-any.whl\n",
            "Collecting oauthlib>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 53.5MB/s \n",
            "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.6MB/s \n",
            "\u001b[?25hCollecting zipp>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/8c/715c54e9e34c0c4820f616a913a7de3337d0cd79074dd1bed4dd840f16ae/zipp-3.4.1-py3-none-any.whl\n",
            "Building wheels for collected packages: termcolor, wrapt\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4832 sha256=6b5e58ebfba5d744229e270cf062728b53e00a72bc40fafcb3bdca4ef9b48372\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68668 sha256=cc1c218361b6aeebfc522a6d8f9a5b0fc756a7c2f2cf8c48cd1dac184b39a73e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
            "Successfully built termcolor wrapt\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, numpy, keras-preprocessing, gast, h5py, wheel, flatbuffers, absl-py, grpcio, termcolor, opt-einsum, google-pasta, werkzeug, pyasn1, pyasn1-modules, cachetools, setuptools, rsa, google-auth, oauthlib, certifi, idna, urllib3, chardet, requests, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, protobuf, typing-extensions, zipp, importlib-metadata, markdown, tensorboard, astunparse, tensorflow-estimator, wrapt, tensorflow-gpu\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Found existing installation: wheel 0.36.2\n",
            "    Uninstalling wheel-0.36.2:\n",
            "      Successfully uninstalled wheel-0.36.2\n",
            "  Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Found existing installation: opt-einsum 3.3.0\n",
            "    Uninstalling opt-einsum-3.3.0:\n",
            "      Successfully uninstalled opt-einsum-3.3.0\n",
            "  Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Found existing installation: pyasn1 0.4.8\n",
            "    Uninstalling pyasn1-0.4.8:\n",
            "      Successfully uninstalled pyasn1-0.4.8\n",
            "  Found existing installation: pyasn1-modules 0.2.8\n",
            "    Uninstalling pyasn1-modules-0.2.8:\n",
            "      Successfully uninstalled pyasn1-modules-0.2.8\n",
            "  Found existing installation: cachetools 4.2.1\n",
            "    Uninstalling cachetools-4.2.1:\n",
            "      Successfully uninstalled cachetools-4.2.1\n",
            "  Found existing installation: setuptools 56.0.0\n",
            "    Uninstalling setuptools-56.0.0:\n",
            "      Successfully uninstalled setuptools-56.0.0\n",
            "  Found existing installation: rsa 4.7.2\n",
            "    Uninstalling rsa-4.7.2:\n",
            "      Successfully uninstalled rsa-4.7.2\n",
            "  Found existing installation: google-auth 1.28.1\n",
            "    Uninstalling google-auth-1.28.1:\n",
            "      Successfully uninstalled google-auth-1.28.1\n",
            "  Found existing installation: oauthlib 3.1.0\n",
            "    Uninstalling oauthlib-3.1.0:\n",
            "      Successfully uninstalled oauthlib-3.1.0\n",
            "  Found existing installation: certifi 2020.12.5\n",
            "    Uninstalling certifi-2020.12.5:\n",
            "      Successfully uninstalled certifi-2020.12.5\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: requests-oauthlib 1.3.0\n",
            "    Uninstalling requests-oauthlib-1.3.0:\n",
            "      Successfully uninstalled requests-oauthlib-1.3.0\n",
            "  Found existing installation: google-auth-oauthlib 0.4.4\n",
            "    Uninstalling google-auth-oauthlib-0.4.4:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.4\n",
            "  Found existing installation: tensorboard-plugin-wit 1.8.0\n",
            "    Uninstalling tensorboard-plugin-wit-1.8.0:\n",
            "      Successfully uninstalled tensorboard-plugin-wit-1.8.0\n",
            "  Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "  Found existing installation: typing-extensions 3.7.4.3\n",
            "    Uninstalling typing-extensions-3.7.4.3:\n",
            "      Successfully uninstalled typing-extensions-3.7.4.3\n",
            "  Found existing installation: zipp 3.4.1\n",
            "    Uninstalling zipp-3.4.1:\n",
            "      Successfully uninstalled zipp-3.4.1\n",
            "  Found existing installation: importlib-metadata 3.10.1\n",
            "    Uninstalling importlib-metadata-3.10.1:\n",
            "      Successfully uninstalled importlib-metadata-3.10.1\n",
            "  Found existing installation: Markdown 3.3.4\n",
            "    Uninstalling Markdown-3.3.4:\n",
            "      Successfully uninstalled Markdown-3.3.4\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: astunparse 1.6.3\n",
            "    Uninstalling astunparse-1.6.3:\n",
            "      Successfully uninstalled astunparse-1.6.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.2 certifi-2020.12.5 chardet-4.0.0 flatbuffers-1.12 gast-0.3.3 google-auth-1.30.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 idna-2.10 importlib-metadata-4.0.1 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.15.8 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.7.2 setuptools-56.0.0 six-1.15.0 tensorboard-2.5.0 tensorboard-data-server-0.6.0 tensorboard-plugin-wit-1.8.0 tensorflow-estimator-2.4.0 tensorflow-gpu-2.4.1 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.4 werkzeug-1.0.1 wheel-0.36.2 wrapt-1.12.1 zipp-3.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "absl",
                  "astunparse",
                  "cachetools",
                  "certifi",
                  "chardet",
                  "flatbuffers",
                  "gast",
                  "google",
                  "grpc",
                  "h5py",
                  "idna",
                  "keras_preprocessing",
                  "numpy",
                  "opt_einsum",
                  "pkg_resources",
                  "pyasn1",
                  "pyasn1_modules",
                  "requests",
                  "rsa",
                  "six",
                  "tensorboard",
                  "tensorflow",
                  "termcolor",
                  "typing_extensions",
                  "urllib3",
                  "wrapt"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GP73R4ExU9F"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from scipy.misc import imread\n",
        "from abc import abstractmethod\n",
        "from __future__ import print_function"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F2PRK-h8quW",
        "outputId": "e418ad20-caf8-4608-d05a-860cad7ca21d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Prompt for input\n",
        "name = input(\"Select a model name \")\n",
        "while True:\n",
        "  dataset = input(\"Select a dataset, either places365_dataset or cifar10_dataset \")\n",
        "  if \"PLACES365\" in dataset.upper():\n",
        "    dataset = 'PLACE365_DATASET'\n",
        "    dataset_path = './dataset/places365'\n",
        "    break\n",
        "  elif 'CIFAR10' in dataset.upper():\n",
        "    dataset = 'CIFAR10_DATASET'\n",
        "    dataset_path = './dataset/cifar10'\n",
        "    break\n",
        "  else:\n",
        "    print(\"Invalid dataset\")\n",
        "  \n",
        "batch_size = int(input(\"Please select a batch size \"))\n",
        "\n",
        "options = {\n",
        "    \"seed\": 100,\n",
        "    \"beta1\": 0.0,\n",
        "    \"name\": name.upper(),\n",
        "    \"mode\": 0,\n",
        "    \"dataset\": dataset,\n",
        "    \"dataset_path\": dataset_path,\n",
        "    \"checkpoints_path\": './checkpoints',\n",
        "    \"color_space\": 'lab',\n",
        "    \"batch_size\": batch_size,\n",
        "    \"l1-weigh\": 100.0,\n",
        "    \"lr\": '3e-4',\n",
        "    \"lr-decay\": True,\n",
        "    \"lr-decay-rate\": 0.1,\n",
        "    \"lr_decay_steps\": '1e4',\n",
        "    \"augment\": True,\n",
        "    \"acc_thresh\": 2.0,\n",
        "    \"gpu_ids\": 0,\n",
        "    \"save\": True,\n",
        "    \"save_interval\": 1000,\n",
        "    \"sample\": True,\n",
        "    \"sample_size\": 8,\n",
        "    \"sample_interval\": 1000,\n",
        "    \"validate\": False,\n",
        "    \"validate_interval\": 0,\n",
        "    \"log\": True,\n",
        "    \"log_interval\": 10,\n",
        "    \"visualize\": True,\n",
        "    \"visualize_window\": 100,\n",
        "    \"test-input\": '',\n",
        "    \"test-output\": '',\n",
        "    \"turing_test_size\": 100,\n",
        "    \"turing_test_delay\": 0,\n",
        "    \"label_smoothing\": 1\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Select a model name Morty\n",
            "Select a dataset, either places365_dataset or cifar10_dataset cifar10_dataset\n",
            "Please select a batch size 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSpIwxlX0yxn"
      },
      "source": [
        "### Driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngYKG5r5y15h",
        "outputId": "6d485065-6ff9-4f07-ad3c-97e5e0153e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Driver\n",
        "\n",
        "# reset tensorflow graph\n",
        "tf.compat.v1.reset_default_graph()\n",
        "# initialize random seed\n",
        "tf.random.set_seed(options[\"seed\"])\n",
        "np.random.seed(options[\"seed\"])\n",
        "random.seed(options[\"seed\"])\n",
        "# create a session environment\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  if options[\"dataset\"] == 'CIFAR10_DATASET':\n",
        "    model = Cifar10Model(sess, options)\n",
        "\n",
        "  elif options[\"dataset\"] == 'PLACES365_DATASET':\n",
        "    model = Places365Model(sess, options)\n",
        "\n",
        "  if not os.path.exists(options[\"checkpoints_path\"]):\n",
        "    os.makedirs(options[\"checkpoints_path\"])\n",
        "\n",
        "  if options[\"log\"]:\n",
        "    open(model.train_log_file, 'w').close()\n",
        "    open(model.test_log_file, 'w').close()\n",
        "  # build the model and initialize\n",
        "  model.build()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  # load model only after global variables initialization\n",
        "  model.load()\n",
        "  if options.mode == 0:\n",
        "    args = vars(options)\n",
        "    print('\\n------------ Options -------------')\n",
        "    with open(os.path.join(options.checkpoints_path, 'options.dat'), 'w') as f:\n",
        "      for k, v in sorted(args.items()):\n",
        "        print('%s: %s' % (str(k), str(v)))\n",
        "        f.write('%s: %s\\n' % (str(k), str(v)))\n",
        "    print('-------------- End ----------------\\n') \n",
        "\n",
        "    model.train()\n",
        "      \n",
        "  elif options[\"mode\"] == 1:\n",
        "      model.test()\n",
        "  else:\n",
        "      model.turing_test()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3ad4dd0ed6ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'CIFAR10_DATASET'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCifar10Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'PLACES365_DATASET'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Cifar10Model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJk5dwX4LJNg"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP4_iitzL57G"
      },
      "source": [
        "CIFAR10_DATASET = 'cifar10'\n",
        "PLACES365_DATASET = 'places365'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "munjCgoxLKzA"
      },
      "source": [
        "class BaseDataset():\n",
        "    def __init__(self, name, path, training=True, augment=True):\n",
        "        self.name = name\n",
        "        self.augment = augment and training\n",
        "        self.training = training\n",
        "        self.path = path\n",
        "        self._data = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __iter__(self):\n",
        "        total = len(self)\n",
        "        start = 0\n",
        "\n",
        "        while start < total:\n",
        "            item = self[start]\n",
        "            start += 1\n",
        "            yield item\n",
        "\n",
        "        raise StopIteration\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        val = self.data[index]\n",
        "        try:\n",
        "            img = imread(val) if isinstance(val, str) else val\n",
        "\n",
        "            # grayscale images\n",
        "            if np.sum(img[:,:,0] - img[:,:,1]) == 0 and np.sum(img[:,:,0] - img[:,:,2]) == 0:\n",
        "                return None\n",
        "\n",
        "            if self.augment and np.random.binomial(1, 0.5) == 1:\n",
        "                img = img[:, ::-1, :]\n",
        "\n",
        "        except:\n",
        "            img = None\n",
        "\n",
        "        return img\n",
        "\n",
        "    def generator(self, batch_size, recusrive=False):\n",
        "        start = 0\n",
        "        total = len(self)\n",
        "\n",
        "        while True:\n",
        "            while start < total:\n",
        "                end = np.min([start + batch_size, total])\n",
        "                items = []\n",
        "\n",
        "                for ix in range(start, end):\n",
        "                    item = self[ix]\n",
        "                    if item is not None:\n",
        "                        items.append(item)\n",
        "\n",
        "                start = end\n",
        "                yield items\n",
        "\n",
        "            if recusrive:\n",
        "                start = 0\n",
        "\n",
        "            else:\n",
        "                raise StopIteration\n",
        "\n",
        "    @property\n",
        "    def data(self):\n",
        "        if len(self._data) == 0:\n",
        "            self._data = self.load()\n",
        "            np.random.shuffle(self._data)\n",
        "\n",
        "        return self._data\n",
        "\n",
        "    @abstractmethod\n",
        "    def load(self):\n",
        "        return []"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-4QgxFVL-In"
      },
      "source": [
        "class Cifar10Dataset(BaseDataset):\n",
        "    def __init__(self, path, training=True, augment=True):\n",
        "        super(Cifar10Dataset, self).__init__(CIFAR10_DATASET, path, training, augment)\n",
        "\n",
        "    def load(self):\n",
        "        data = []\n",
        "        if self.training:\n",
        "            for i in range(1, 6):\n",
        "                filename = '{}/data_batch_{}'.format(self.path, i)\n",
        "                batch_data = unpickle(filename)\n",
        "                if len(data) > 0:\n",
        "                    data = np.vstack((data, batch_data[b'data']))\n",
        "                else:\n",
        "                    data = batch_data[b'data']\n",
        "\n",
        "        else:\n",
        "            filename = '{}/test_batch'.format(self.path)\n",
        "            batch_data = unpickle(filename)\n",
        "            data = batch_data[b'data']\n",
        "\n",
        "        w = 32\n",
        "        h = 32\n",
        "        s = w * h\n",
        "        data = np.array(data)\n",
        "        data = np.dstack((data[:, :s], data[:, s:2 * s], data[:, 2 * s:]))\n",
        "        data = data.reshape((-1, w, h, 3))\n",
        "        return data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9nHcs8503-k"
      },
      "source": [
        "### Helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcsc9ogM0HWy"
      },
      "source": [
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise ('Boolean value expected.')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGA96OFazXZO"
      },
      "source": [
        "def parse(input):\n",
        "  opt = input\n",
        "  os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpu_ids\n",
        "  opt.color_space = opt.color_space.upper()\n",
        "  opt.training = opt.mode == 1\n",
        "\n",
        "  if opt.seed == 0:\n",
        "    opt.seed = random.randint(0, 2**31 - 1)\n",
        "\n",
        "  if opt.dataset_path == './dataset':\n",
        "    opt.dataset_path += ('/' + opt.dataset)\n",
        "\n",
        "  if opt.checkpoints_path == './checkpoints':\n",
        "    opt.checkpoints_path += ('/' + opt.dataset)\n",
        "\n",
        "    return opt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7AjDhlq1YwV"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9F-MP9bJe5v"
      },
      "source": [
        "def stitch_images(grayscale, original, pred):\n",
        "    gap = 5\n",
        "    width, height = original[0][:, :, 0].shape\n",
        "    img_per_row = 2 if width > 200 else 4\n",
        "    img = Image.new('RGB', (width * img_per_row * 3 + gap * (img_per_row - 1), height * int(len(original) / img_per_row)))\n",
        "\n",
        "    grayscale = np.array(grayscale).squeeze()\n",
        "    original = np.array(original)\n",
        "    pred = np.array(pred)\n",
        "\n",
        "    for ix in range(len(original)):\n",
        "        xoffset = int(ix % img_per_row) * width * 3 + int(ix % img_per_row) * gap\n",
        "        yoffset = int(ix / img_per_row) * height\n",
        "        im1 = Image.fromarray(grayscale[ix])\n",
        "        im2 = Image.fromarray(original[ix])\n",
        "        im3 = Image.fromarray((pred[ix] * 255).astype(np.uint8))\n",
        "        img.paste(im1, (xoffset, yoffset))\n",
        "        img.paste(im2, (xoffset + width, yoffset))\n",
        "        img.paste(im3, (xoffset + width + width, yoffset))\n",
        "\n",
        "    return img\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEE0LX0PJfnW"
      },
      "source": [
        "def create_dir(dir):\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "\n",
        "    return dir"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlLFjIcBJfg0"
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_RoZnlEJfZK"
      },
      "source": [
        "def moving_average(data, window_width):\n",
        "    cumsum_vec = np.cumsum(np.insert(data, 0, 0))\n",
        "    ma_vec = (cumsum_vec[window_width:] - cumsum_vec[:-window_width]) / window_width\n",
        "    return ma_vec"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLckq20lJfQG"
      },
      "source": [
        "def imshow(img, title=''):\n",
        "    fig = plt.gcf()\n",
        "    fig.canvas.set_window_title(title)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img, interpolation='none')\n",
        "    plt.show()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTAqlwtdJsTy"
      },
      "source": [
        "def imsave(img, path):\n",
        "    im = Image.fromarray(np.array(img).astype(np.uint8).squeeze())\n",
        "    im.save(path)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39WEh-A7JsiN"
      },
      "source": [
        "def turing_test(real_img, fake_img, delay=0):\n",
        "    height, width, _ = real_img.shape\n",
        "    imgs = np.array([real_img, (fake_img * 255).astype(np.uint8)])\n",
        "    real_index = np.random.binomial(1, 0.5)\n",
        "    fake_index = (real_index + 1) % 2\n",
        "\n",
        "    img = Image.new('RGB', (2 + width * 2, height))\n",
        "    img.paste(Image.fromarray(imgs[real_index]), (0, 0))\n",
        "    img.paste(Image.fromarray(imgs[fake_index]), (2 + width, 0))\n",
        "\n",
        "    img.success = 0\n",
        "\n",
        "    def onclick(event):\n",
        "        if event.xdata is not None:\n",
        "            if event.x < width and real_index == 0:\n",
        "                img.success = 1\n",
        "\n",
        "            elif event.x > width and real_index == 1:\n",
        "                img.success = 1\n",
        "\n",
        "        plt.gcf().canvas.stop_event_loop()\n",
        "\n",
        "    plt.ion()\n",
        "    plt.gcf().canvas.mpl_connect('button_press_event', onclick)\n",
        "    plt.title('click on the real image')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img, interpolation='none')\n",
        "    plt.show()\n",
        "    plt.draw()\n",
        "    plt.gcf().canvas.start_event_loop(delay)\n",
        "\n",
        "    return img.success"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90FAEkHoJsyl"
      },
      "source": [
        "def visualize(train_log_file, test_log_file, window_width, title=''):\n",
        "    train_data = np.loadtxt(train_log_file)\n",
        "    test_data = np.loadtxt(test_log_file)\n",
        "\n",
        "    if len(train_data.shape) < 2:\n",
        "        return\n",
        "\n",
        "    if len(train_data) < window_width:\n",
        "        window_width = len(train_data) - 1\n",
        "\n",
        "    fig = plt.gcf()\n",
        "    fig.canvas.set_window_title(title)\n",
        "\n",
        "    plt.ion()\n",
        "    plt.subplot('121')\n",
        "    plt.cla()\n",
        "    if len(train_data) > 1:\n",
        "        plt.plot(moving_average(train_data[:, 8], window_width))\n",
        "    plt.title('train')\n",
        "\n",
        "    plt.subplot('122')\n",
        "    plt.cla()\n",
        "    if len(test_data) > 1:\n",
        "        plt.plot(test_data[:, 8])\n",
        "    plt.title('test')\n",
        "\n",
        "    plt.show()\n",
        "    plt.draw()\n",
        "    plt.pause(.01)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EEgGNvIJ7hh"
      },
      "source": [
        "### Ops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNh-LQlFKDmi"
      },
      "source": [
        "COLORSPACE_RGB = 'RGB'\n",
        "COLORSPACE_LAB = 'LAB'\n",
        "#tf.nn.softmax_cross_entropy_with_logits_v2"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDFVi6SYKAkM"
      },
      "source": [
        "def conv2d(inputs, filters, name, kernel_size=4, strides=2, bnorm=True, activation=None, seed=None):\n",
        "    \"\"\"\n",
        "    Creates a conv2D block\n",
        "    \"\"\"\n",
        "    initializer=tf.variance_scaling_initializer(seed=seed)\n",
        "    res = tf.layers.conv2d(\n",
        "        name=name,\n",
        "        inputs=inputs,\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=initializer)\n",
        "\n",
        "    if bnorm:\n",
        "        res = tf.layers.batch_normalization(inputs=res, name='bn_' + name, training=True)\n",
        "\n",
        "    # activation after batch-norm\n",
        "    if activation is not None:\n",
        "        res = activation(res)\n",
        "\n",
        "    return res"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5d84dahKAeM"
      },
      "source": [
        "def conv2d_transpose(inputs, filters, name, kernel_size=4, strides=2, bnorm=True, activation=None, seed=None):\n",
        "    \"\"\"\n",
        "    Creates a conv2D-transpose block\n",
        "    \"\"\"\n",
        "    initializer=tf.variance_scaling_initializer(seed=seed)\n",
        "    res = tf.layers.conv2d_transpose(\n",
        "        name=name,\n",
        "        inputs=inputs,\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=initializer)\n",
        "\n",
        "    if bnorm:\n",
        "        res = tf.layers.batch_normalization(inputs=res, name='bn_' + name, training=True)\n",
        "\n",
        "    # activation after batch-norm\n",
        "    if activation is not None:\n",
        "        res = activation(res)\n",
        "\n",
        "    return res"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ1K6xBEKAYw"
      },
      "source": [
        "def pixelwise_accuracy(img_real, img_fake, colorspace, thresh):\n",
        "    \"\"\"\n",
        "    Measures the accuracy of the colorization process by comparing pixels\n",
        "    \"\"\"\n",
        "    img_real = postprocess(img_real, colorspace, COLORSPACE_LAB)\n",
        "    img_fake = postprocess(img_fake, colorspace, COLORSPACE_LAB)\n",
        "\n",
        "    diffL = tf.abs(tf.round(img_real[..., 0]) - tf.round(img_fake[..., 0]))\n",
        "    diffA = tf.abs(tf.round(img_real[..., 1]) - tf.round(img_fake[..., 1]))\n",
        "    diffB = tf.abs(tf.round(img_real[..., 2]) - tf.round(img_fake[..., 2]))\n",
        "\n",
        "    # within %thresh of the original\n",
        "    predL = tf.cast(tf.less_equal(diffL, 1 * thresh), tf.float64)        # L: [0, 100]\n",
        "    predA = tf.cast(tf.less_equal(diffA, 2.2 * thresh), tf.float64)      # A: [-110, 110]\n",
        "    predB = tf.cast(tf.less_equal(diffB, 2.2 * thresh), tf.float64)      # B: [-110, 110]\n",
        "\n",
        "    # all three channels are within the threshold\n",
        "    pred = predL * predA * predB\n",
        "\n",
        "    return tf.reduce_mean(pred)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n-SkClBKASx"
      },
      "source": [
        "def preprocess(img, colorspace_in, colorspace_out):\n",
        "    if colorspace_out.upper() == COLORSPACE_RGB:\n",
        "        if colorspace_in == COLORSPACE_LAB:\n",
        "            img = lab_to_rgb(img)\n",
        "\n",
        "        # [0, 1] => [-1, 1]\n",
        "        img = (img / 255.0) * 2 - 1\n",
        "\n",
        "    elif colorspace_out.upper() == COLORSPACE_LAB:\n",
        "        if colorspace_in == COLORSPACE_RGB:\n",
        "            img = rgb_to_lab(img / 255.0)\n",
        "\n",
        "        L_chan, a_chan, b_chan = tf.unstack(img, axis=3)\n",
        "\n",
        "        # L: [0, 100] => [-1, 1]\n",
        "        # A, B: [-110, 110] => [-1, 1]\n",
        "        img = tf.stack([L_chan / 50 - 1, a_chan / 110, b_chan / 110], axis=3)\n",
        "\n",
        "    return img"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZcMVbpNKANL"
      },
      "source": [
        "def postprocess(img, colorspace_in, colorspace_out):\n",
        "    if colorspace_in.upper() == COLORSPACE_RGB:\n",
        "        # [-1, 1] => [0, 1]\n",
        "        img = (img + 1) / 2\n",
        "\n",
        "        if colorspace_out == COLORSPACE_LAB:\n",
        "            img = rgb_to_lab(img)\n",
        "\n",
        "    elif colorspace_in.upper() == COLORSPACE_LAB:\n",
        "        L_chan, a_chan, b_chan = tf.unstack(img, axis=3)\n",
        "\n",
        "        # L: [-1, 1] => [0, 100]\n",
        "        # A, B: [-1, 1] => [-110, 110]\n",
        "        img = tf.stack([(L_chan + 1) / 2 * 100, a_chan * 110, b_chan * 110], axis=3)\n",
        "\n",
        "        if colorspace_out == COLORSPACE_RGB:\n",
        "            img = lab_to_rgb(img)\n",
        "\n",
        "    return img"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n537DjJYKAFc"
      },
      "source": [
        "def rgb_to_lab(srgb):\n",
        "    # based on https://github.com/torch/image/blob/9f65c30167b2048ecbe8b7befdc6b2d6d12baee9/generic/image.c\n",
        "    with tf.name_scope(\"rgb_to_lab\"):\n",
        "        srgb_pixels = tf.reshape(srgb, [-1, 3])\n",
        "\n",
        "        with tf.name_scope(\"srgb_to_xyz\"):\n",
        "            linear_mask = tf.cast(srgb_pixels <= 0.04045, dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(srgb_pixels > 0.04045, dtype=tf.float32)\n",
        "            rgb_pixels = (srgb_pixels / 12.92 * linear_mask) + (((srgb_pixels + 0.055) / 1.055) ** 2.4) * exponential_mask\n",
        "            rgb_to_xyz = tf.constant([\n",
        "                #    X        Y          Z\n",
        "                [0.412453, 0.212671, 0.019334],  # R\n",
        "                [0.357580, 0.715160, 0.119193],  # G\n",
        "                [0.180423, 0.072169, 0.950227],  # B\n",
        "            ])\n",
        "            xyz_pixels = tf.matmul(rgb_pixels, rgb_to_xyz)\n",
        "\n",
        "        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
        "        with tf.name_scope(\"xyz_to_cielab\"):\n",
        "\n",
        "            # normalize for D65 white point\n",
        "            xyz_normalized_pixels = tf.multiply(xyz_pixels, [1 / 0.950456, 1.0, 1 / 1.088754])\n",
        "\n",
        "            epsilon = 6 / 29\n",
        "            linear_mask = tf.cast(xyz_normalized_pixels <= (epsilon**3), dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(xyz_normalized_pixels > (epsilon**3), dtype=tf.float32)\n",
        "            fxfyfz_pixels = (xyz_normalized_pixels / (3 * epsilon**2) + 4 / 29) * linear_mask + (xyz_normalized_pixels ** (1 / 3)) * exponential_mask\n",
        "\n",
        "            # convert to lab\n",
        "            fxfyfz_to_lab = tf.constant([\n",
        "                #  l       a       b\n",
        "                [0.0, 500.0, 0.0],  # fx\n",
        "                [116.0, -500.0, 200.0],  # fy\n",
        "                [0.0, 0.0, -200.0],  # fz\n",
        "            ])\n",
        "            lab_pixels = tf.matmul(fxfyfz_pixels, fxfyfz_to_lab) + tf.constant([-16.0, 0.0, 0.0])\n",
        "\n",
        "        return tf.reshape(lab_pixels, tf.shape(srgb))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4_LOFJUJ_3d"
      },
      "source": [
        "def lab_to_rgb(lab):\n",
        "    with tf.name_scope(\"lab_to_rgb\"):\n",
        "        lab_pixels = tf.reshape(lab, [-1, 3])\n",
        "\n",
        "        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
        "        with tf.name_scope(\"cielab_to_xyz\"):\n",
        "            # convert to fxfyfz\n",
        "            lab_to_fxfyfz = tf.constant([\n",
        "                #   fx      fy        fz\n",
        "                [1 / 116.0, 1 / 116.0, 1 / 116.0],  # l\n",
        "                [1 / 500.0, 0.0, 0.0],  # a\n",
        "                [0.0, 0.0, -1 / 200.0],  # b\n",
        "            ])\n",
        "            fxfyfz_pixels = tf.matmul(lab_pixels + tf.constant([16.0, 0.0, 0.0]), lab_to_fxfyfz)\n",
        "\n",
        "            # convert to xyz\n",
        "            epsilon = 6 / 29\n",
        "            linear_mask = tf.cast(fxfyfz_pixels <= epsilon, dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(fxfyfz_pixels > epsilon, dtype=tf.float32)\n",
        "            xyz_pixels = (3 * epsilon**2 * (fxfyfz_pixels - 4 / 29)) * linear_mask + (fxfyfz_pixels ** 3) * exponential_mask\n",
        "\n",
        "            # denormalize for D65 white point\n",
        "            xyz_pixels = tf.multiply(xyz_pixels, [0.950456, 1.0, 1.088754])\n",
        "\n",
        "        with tf.name_scope(\"xyz_to_srgb\"):\n",
        "            xyz_to_rgb = tf.constant([\n",
        "                #     r           g          b\n",
        "                [3.2404542, -0.9692660, 0.0556434],  # x\n",
        "                [-1.5371385, 1.8760108, -0.2040259],  # y\n",
        "                [-0.4985314, 0.0415560, 1.0572252],  # z\n",
        "            ])\n",
        "            rgb_pixels = tf.matmul(xyz_pixels, xyz_to_rgb)\n",
        "            # avoid a slightly negative number messing up the conversion\n",
        "            rgb_pixels = tf.clip_by_value(rgb_pixels, 0.0, 1.0)\n",
        "            linear_mask = tf.cast(rgb_pixels <= 0.0031308, dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(rgb_pixels > 0.0031308, dtype=tf.float32)\n",
        "            srgb_pixels = (rgb_pixels * 12.92 * linear_mask) + ((rgb_pixels ** (1 / 2.4) * 1.055) - 0.055) * exponential_mask\n",
        "\n",
        "        return tf.reshape(srgb_pixels, tf.shape(lab))"
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}